{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK\n",
    "\n",
    "Consider the large scale Twitter friendship dataset available at Twitter Friends (kaggle.com) of full\n",
    "Twitter user profile data (40K users), including friendship relationship. We want to explore this\n",
    "friendship relation to construct a network graph where User IDs are nodes and a directed edge from\n",
    "IDx to IDy if IDy is listed as a friend of IDx.\n",
    "\n",
    "1. Use NetworkX to display the corresponding network, suggest an approach to visualize this\n",
    "dense network using visualization tool of your choice (NetworkX is not ideal for dense\n",
    "graphs). Save the adjacency matrix of this graph in a separate file.\n",
    "2. Write a script that uses NetworkX functions to calculate diameter, average clustering\n",
    "coefficient and average path length of the network.\n",
    "3. Write a script that plots the degree centrality distribution and closeness centrality\n",
    "distribution.\n",
    "4. We want to test the extent to which the centrality distributions in 3) fit a power law\n",
    "distribution. You may inspire from the implementation in powerlaw · PyPI of the power-law\n",
    "distribution, or can use alternative one of your choice. It is important to quantify the\n",
    "goodness of fit using p-value. Typically, when p-value is greater than 10%, we can state that\n",
    "power-law is a plausible fit to the (distribution) data.\n",
    "5. Write a script that calculates the number of triangles in the network.\n",
    "6. Write a script that identifies the largest strongly connected component, second largest, third\n",
    "largest. Display each component (If there is more than one component for a given case,\n",
    "then draw one at random).\n",
    "7. Write a script that calculates the shortest distance between any pair of the strongly\n",
    "connected components in 5). Present the result in a table. Comment on the separability\n",
    "between the above components.\n",
    "8. We want to identify relevant communities from the subnetwork graph corresponding to the\n",
    "largest, second largest and third largest component. For this purpose, use Label propagation\n",
    "algorithm implementation in NetworkX to identify the main communities. Write a script\n",
    "that uses different color for each community and visualize the above graph with the detected\n",
    "communities. Use the appropriate function in NetworkX to compute the separation among\n",
    "the various communities and any other related quality measures. Comment on the quality of\n",
    "the partition.\n",
    "9. We want to evaluate the evolution of the triangles (transitivity relation in the network). For\n",
    "this purpose, we consider time increment as an accumulation of one thousand successive\n",
    "rows in the original dataset. Suggest a script that calculates the evolution of the proportion\n",
    "of triangles (number of triangles over the total number of nodes up to time t, basically t=1,\n",
    "2,…,40), and draws the corresponding graph.\n",
    "10. From 9), write a script that identifies instances of triplet nodes A, B, C such at time t A is\n",
    "connected to B and B is connected to C but A is not connected to C, while in time t+1, A\n",
    "becomes connected to C. Check for theses instances whether the link prediction using\n",
    "common neighbor (probability A being connected to C increases with the number of\n",
    "common neighbours between A and B). Write the result in a table.\n",
    "11. Suggest appropriate literature to comment on the various findings and explore the limitation\n",
    "of the reasoning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains the following columns:\n",
    "- avatar: URL to the profile picture\n",
    "- followerCount: the number of followers of this user\n",
    "- friendsCount: the number of people following this user\n",
    "- friendName: stores the @name (without the '@') of the user (beware this name can be changed by the user)\n",
    "- id: user ID, this number can not change (you can retrieve screen name with this service: https://tweeterid.com/)\n",
    "- friends: the list of IDs the user follows (data stored is IDs of users followed by this user)\n",
    "- lang: the language declared by the user (in this dataset there is only \"en\" (english))\n",
    "- lastSeen: the time stamp of the date when this user have post his last tweet\n",
    "- tags: the hashtags (with or without #) used by the user. It's the \"trending topic\" the user tweeted about\n",
    "- tweetID: ID of the last tweet posted by this user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import igraph as ig\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import powerlaw\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import json\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import networkx.algorithms.community as nx_comm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset from [here](https://www.kaggle.com/datasets/hwassner/TwitterFriends/download?datasetVersionNumber=4)\n",
    "\n",
    "And unzip the file to get the `data.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:11: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\S'\n",
      "C:\\Users\\joona\\AppData\\Local\\Temp\\ipykernel_36896\\4194049718.py:11: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  df = pd.read_csv('data.csv', sep=',(?=\\S)', engine='python')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes:  40000\n",
      "Number of edges:  32887158\n"
     ]
    }
   ],
   "source": [
    "# open the csv file\n",
    "with open('data.csv', 'r') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "# remove the first line\n",
    "data = data[1:]\n",
    "\n",
    "\n",
    "\n",
    "# Read the data into a pandas dataframe and remove the quotes\n",
    "df = pd.read_csv('data.csv', sep=',(?=\\S)', engine='python')\n",
    "\n",
    "def delete_quotes(x):\n",
    "    return x[1:-1]\n",
    "\n",
    "for column in [\"id\", \"screenName\", \"avatar\", \"lang\", \"tweetId\"]:\n",
    "    df[column] = df[column].apply(delete_quotes)\n",
    "\n",
    "for column in [\"tags\", \"friends\"]:\n",
    "    df[column] = df[column].apply(lambda x: json.loads(x))\n",
    "\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 40000 Number of edges: 184548\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1. Use NetworkX to display the corresponding network, suggest an approach to visualize this\n",
    "dense network using visualization tool of your choice (NetworkX is not ideal for dense\n",
    "graphs). Save the adjacency matrix of this graph in a separate file.\n",
    "\n",
    "TODO: Alternative visualization\n",
    "'''\n",
    "\n",
    "def create_directed_graph(df):\n",
    "    \"\"\"\n",
    "    Use directed graph to represent the network\n",
    "\n",
    "    We want to explore this friendship relation to construct a network graph where User IDs are nodes\n",
    "    and a directed edge from IDx to IDy if IDy is listed as a friend of IDx.\n",
    "    \"\"\"\n",
    "    # Create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add nodes from 'id' column\n",
    "    G.add_nodes_from(df['id'])\n",
    "\n",
    "    # Add edges from 'id' to 'friends' if 'friend' is also a node in the graph\n",
    "    for _, row in df.iterrows():\n",
    "        id = row['id']\n",
    "        friends = row['friends']\n",
    "        for friend in friends:\n",
    "            if friend in G:\n",
    "                G.add_edge(id, friend)\n",
    "\n",
    "    print(f'Number of nodes: {G.number_of_nodes()} Number of edges: {G.number_of_edges()}')\n",
    "    return G\n",
    "\n",
    "def save_adjacency_matrix(G, filename):\n",
    "    adjacency_matrix = nx.adjacency_matrix(G)\n",
    "    with open(filename, 'wb') as f:\n",
    "        np.save(f, adjacency_matrix)\n",
    "\n",
    "\n",
    "G = create_directed_graph(df)\n",
    "save_adjacency_matrix(G, 'adjacency_matrix.npy')\n",
    "\n",
    "# create a visualization using Gephi\n",
    "# export the graph to a GEXF file\n",
    "# nx.write_gexf(G, 'graph.gexf')\n",
    "# print('Graph exported to graph.gexf')\n",
    "# open the file in Gephi and visualize the graph\n",
    "\n",
    "# create a visualization using Cytoscape (GraphML)\n",
    "# nx.write_graphml(G, \"twitter_dataset.graphml\")\n",
    "\n",
    "\n",
    "# Save the adjacency matrix\n",
    "#adj_matrix = nx.adjacency_matrix(G)\n",
    "#scipy.sparse.save_npz('adj_matrix.npz', adj_matrix)\n",
    "#print(f\"Adjacency matrix: {adj_matrix}\")\n",
    "\n",
    "# Visualize the graph using NetworkX (not ideal for dense graphs)\n",
    "# draw only the first 200 nodes\n",
    "# G_copy = G.subgraph(list(G.nodes())[:200])\n",
    "# pos = nx.spring_layout(G_copy)\n",
    "# plt.figure(figsize=(20, 20))\n",
    "# nx.draw(G_copy, pos, with_labels=True, labels=nx.get_node_attributes(G_copy, 'screenName'), node_size=80, font_size=6, linewidths=0.1, edge_color='gray')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# visualize the graph using a random layout\n",
    "# fig, ax = plt.subplots(figsize=(15, 9))\n",
    "# ax.axis('off')\n",
    "# plot_options = {\"node_size\": 10, \"with_labels\": False, \"width\": 0.15}\n",
    "# nx.draw_networkx(G, pos=nx.random_layout(G), ax=ax, **plot_options)\n",
    "\n",
    "# Different ways to visualize the graph\n",
    "#G_copy = G.subgraph(list(G.nodes())[:10000])\n",
    "\n",
    "# Plot using igraph with Kamada-Kawai layout\n",
    "#H = ig.Graph.from_networkx(G)\n",
    "#layout = H.layout(\"kk\")\n",
    "#ig.plot(H, layout=layout)\n",
    "\n",
    "#pos = nx.spring_layout(G_copy, iterations=15, seed=1721)\n",
    "#fig, ax = plt.subplots(figsize=(15, 9))\n",
    "#ax.axis(\"off\")\n",
    "#nx.draw_networkx(G_copy, pos=pos, ax=ax, **plot_options)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2. Write a script that uses NetworkX functions to calculate diameter, average clustering\n",
    "coefficient and average path length of the network.\n",
    "'''\n",
    "\n",
    "def calculate_metrics(G):\n",
    "    shortest_path_lengths = dict(nx.all_pairs_shortest_path_length(G))\n",
    "\n",
    "    # Calculate diameter\n",
    "    diameter = max([max(lengths.values()) for lengths in shortest_path_lengths.values()])\n",
    "    print(f\"Diameter: {diameter}\")\n",
    "\n",
    "\n",
    "    # Calculate average path length for all the components\n",
    "    avg_path_lengths = []\n",
    "    for lengths in shortest_path_lengths.values():\n",
    "        avg_path_lengths.append(np.mean(list(lengths.values())))\n",
    "    avg_path_length = np.mean(avg_path_lengths)\n",
    "    print(f\"Average Path Length: {avg_path_length}\")\n",
    "\n",
    "    # Calculate average clustering coefficient\n",
    "    avg_clustering_coefficient = nx.average_clustering(G)\n",
    "    print(f\"Average Clustering Coefficient: {avg_clustering_coefficient}\")\n",
    "\n",
    "    # Calculate degree distribution\n",
    "    degrees = dict(G.degree())\n",
    "    degree_values = list(degrees.values())\n",
    "    degree_values = np.array(degree_values)\n",
    "    degree_values = degree_values[degree_values > 0]\n",
    "    degree_values = np.log(degree_values)\n",
    "    fit = powerlaw.Fit(degree_values)\n",
    "    alpha = fit.alpha\n",
    "    print(f\"ALPHA: {alpha}\")\n",
    "    print(f\"XMIN: {fit.power_law.xmin}\")\n",
    "    print(f\"SIGMA: {fit.power_law.sigma}\")\n",
    "    print(f\"XMAX: {fit.power_law.xmax}\")\n",
    "\n",
    "\n",
    "calculate_metrics(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3. Write a script that plots the degree centrality distribution and closeness centrality\n",
    "distribution.\n",
    "'''\n",
    "\n",
    "def plot_degree_centrality(G):\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    closeness_centrality = nx.closeness_centrality(G)\n",
    "\n",
    "\n",
    "    # Plot degree centrality distribution and closeness centrality distribution\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    ax[0].hist(degree_centrality.values(), bins=100, color='blue', edgecolor='black')\n",
    "    ax[0].set_title(\"Degree Centrality Distribution\")\n",
    "    ax[0].set_xlabel(\"Degree Centrality\")\n",
    "    ax[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "    ax[1].hist(closeness_centrality.values(), bins=100, color='blue', edgecolor='black')\n",
    "    ax[1].set_title(\"Closeness Centrality Distribution\")\n",
    "    ax[1].set_xlabel(\"Closeness Centrality\")\n",
    "    ax[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_degree_centrality(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "4. We want to test the extent to which the centrality distributions in 3) fit a power law\n",
    "distribution. You may inspire from the implementation in powerlaw · PyPI of the power-law\n",
    "distribution, or can use alternative one of your choice. It is important to quantify the\n",
    "goodness of fit using p-value. Typically, when p-value is greater than 10%, we can state that\n",
    "power-law is a plausible fit to the (distribution) data.\n",
    "'''\n",
    "\n",
    "def fit_power_law(G):\n",
    "    # Calculate degree centrality and closeness centrality\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    closeness_centrality = nx.closeness_centrality(G)\n",
    "\n",
    "    # Fit the degree centrality distribution to a power-law distribution and compare with lognormal\n",
    "    cent_values = list(degree_centrality.values())\n",
    "    # print(cent_values)\n",
    "    fit_degree = powerlaw.Fit(cent_values)\n",
    "    R, p = fit_degree.distribution_compare('power_law', 'lognormal')\n",
    "    print(f\"Degree Centrality: p-value = {p}\")\n",
    "    if p > 0.1:\n",
    "        print(\"Power-law is a plausible fit to degree centrality data\")\n",
    "    else:\n",
    "        print(\"Power-law is not a plausible fit to degree centrality data\")\n",
    "\n",
    "    # Fit the closeness centrality distribution to a power-law distribution and compare with lognormal\n",
    "    closeness_values = list(closeness_centrality.values())\n",
    "    # print(closeness_values)\n",
    "    fit_closeness = powerlaw.Fit(closeness_values)\n",
    "    R, p = fit_closeness.distribution_compare('power_law', 'lognormal')\n",
    "    print(f\"Closeness Centrality: p-value = {p}\")\n",
    "    if p > 0.1:\n",
    "        print(\"Power-law is a plausible fit to closeness centrality data\")\n",
    "    else:\n",
    "        print(\"Power-law is not a plausible fit to closeness centrality data\")\n",
    "\n",
    "fit_power_law(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5. Write a script that calculates the number of triangles in the network.\n",
    "'''\n",
    "\n",
    "def count_triangles(G):\n",
    "    triangles = nx.triangles(G)\n",
    "    all_triangles = sum(triangles.values())\n",
    "    # Each triangle is counted for all its three nodes\n",
    "    # Divide by 3 to get the number of triangles in the graph\n",
    "    num_triangles = all_triangles // 3\n",
    "    if num_triangles == 0:\n",
    "        print(\"No triangles found in the graph\")\n",
    "    else:\n",
    "        print(f\"Number of triangles: {num_triangles}\")\n",
    "\n",
    "undirected_G = G.to_undirected()\n",
    "count_triangles(undirected_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "6. Write a script that identifies the largest strongly connected component, second largest, third\n",
    "'''\n",
    "\n",
    "def find_strongly_connected_components(G):\n",
    "\n",
    "    connected_components = sorted(nx.strongly_connected_components(G), key=len, reverse=True)\n",
    "    strongly_connected = connected_components[:3]\n",
    "    for i, component in enumerate(strongly_connected):\n",
    "        subgraph = G.subgraph(component)\n",
    "        print(f\"Strongly connected component {i+1} has {subgraph.number_of_nodes()} nodes and {subgraph.number_of_edges()} edges\")\n",
    "        if i == 2:\n",
    "            break\n",
    "    return connected_components\n",
    "\n",
    "scc = find_strongly_connected_components(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "7. Write a script that calculates the shortest distance between any pair of the strongly\n",
    "connected components in 5). Present the result in a table. Comment on the separability\n",
    "between the above components.\n",
    "\n",
    "TODO: check if this is correct\n",
    "'''\n",
    "\n",
    "def shortest_distance_between_components(G, components):\n",
    "    shortest_distances = []\n",
    "    for i in range(len(components)):\n",
    "        for j in range(i+1, len(components)):\n",
    "            component_i = list(components[i])\n",
    "            component_j = list(components[j])\n",
    "            shortest_distance = np.inf\n",
    "            for node_i in component_i:\n",
    "                for node_j in component_j:\n",
    "                    if nx.has_path(G, node_i, node_j):\n",
    "                        distance = nx.shortest_path_length(G, node_i, node_j)\n",
    "                        if distance < shortest_distance:\n",
    "                            shortest_distance = distance\n",
    "            shortest_distances.append((i+1, j+1, shortest_distance))\n",
    "    return shortest_distances\n",
    "\n",
    "\n",
    "def present_results_in_table(shortest_distances):\n",
    "    df = pd.DataFrame(shortest_distances, columns=['Component 1', 'Component 2', 'Shortest Distance'])\n",
    "    return df\n",
    "\n",
    "shortest_distances = shortest_distance_between_components(G, scc)\n",
    "df = present_results_in_table(shortest_distances)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the shortest distance between any pair of components is large, it indicates that these components are well separated.\n",
    "\n",
    "If the shortest distance is small, it suggests that these components are closely connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "8. We want to identify relevant communities from the subnetwork graph corresponding to the\n",
    "largest, second largest and third largest component. For this purpose, use Label propagation\n",
    "algorithm implementation in NetworkX to identify the main communities. Write a script\n",
    "that uses different color for each community and visualize the above graph with the detected\n",
    "communities. Use the appropriate function in NetworkX to compute the separation among\n",
    "the various communities and any other related quality measures. Comment on the quality of\n",
    "the partition.\n",
    "\n",
    "TODO: Write a script that uses different color for each community and visualize the above graph with the detected\n",
    "    communities. Use the appropriate function in NetworkX to compute the separation among\n",
    "    the various communities and any other related quality measures. Comment on the quality of\n",
    "    the partition.\n",
    "'''\n",
    "\n",
    "def plot_communities(G, node_groups):\n",
    "    colors = [\n",
    "        f\"#{''.join([rd.choice('0123456789ABCDEF') for _ in range(6)])}\"\n",
    "        for _ in range(len(node_groups))\n",
    "    ]\n",
    "    color_map = []\n",
    "    for node in G:\n",
    "        found = False\n",
    "        for i, node_group in enumerate(node_groups):\n",
    "            if node in node_group:\n",
    "                color_map.append(colors[i % len(colors)])\n",
    "                found = True\n",
    "        if not found:\n",
    "            color_map.append(\"grey\")\n",
    "    nx.draw(G, node_color=color_map, with_labels=True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def detect_communities(G):\n",
    "    connected_components = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "    for i, component in enumerate(connected_components):\n",
    "        subgraph = G.subgraph(component)\n",
    "        communities_generator = nx_comm.asyn_lpa_communities(subgraph)\n",
    "        communities = list(communities_generator)  # Convert generator to list\n",
    "        community_map = {}\n",
    "        for j, community in enumerate(communities):\n",
    "            for node in community:\n",
    "                community_map[node] = j\n",
    "\n",
    "        # Use a colormap to assign a unique color to each community\n",
    "        #num_communities = max(community_map.values()) + 1\n",
    "        #cmap = cm.get_cmap('rainbow', num_communities)\n",
    "        #colors = [cmap(community_map[node]) for node in subgraph.nodes()]\n",
    "        plot_communities(subgraph, communities)\n",
    "\n",
    "        #pos = nx.spring_layout(subgraph)\n",
    "        #plt.figure(figsize=(15, 9))\n",
    "        #nx.draw(subgraph, pos, node_color=colors, with_labels=False, node_size=10, edge_color='gray')\n",
    "        #plt.title(f\"Strongly connected component {i+1}\")\n",
    "        #plt.show()\n",
    "\n",
    "        # Compute the modularity of the partition\n",
    "        modularity = nx_comm.modularity(subgraph, communities)\n",
    "        print(f\"Modularity of the partition: {modularity}\")\n",
    "\n",
    "        # Compute the partition_quality\n",
    "        coverage, performance = nx_comm.partition_quality(subgraph, communities)\n",
    "        print(f\"Coverage: {coverage}\", f\"Performance: {performance}\")\n",
    "\n",
    "\n",
    "        if i == 2:\n",
    "            break\n",
    "\n",
    "detect_communities(undirected_G)\n",
    "\n",
    "def print_performance_metrics(G, communities):\n",
    "\n",
    "    print(\"Modularity:\", nx_comm.modularity(G, communities))\n",
    "    coverage, performance = nx_comm.partition_quality(G, communities)\n",
    "    print(\"Coverage:\", coverage)\n",
    "    print(\"Performance:\", performance)\n",
    "    print(\"\")\n",
    "\n",
    "def label_propagation_identification(G, components):\n",
    "    largest_components = components[:3]\n",
    "    for component in largest_components:\n",
    "        subgraph = G.subgraph(component)\n",
    "        communities = list(nx_comm.label_propagation_communities(subgraph))\n",
    "        print_performance_metrics(subgraph, communities)\n",
    "        plot_communities(subgraph, communities)\n",
    "\n",
    "\n",
    "#label_propagation_identification(undirected_G, scc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "9. We want to evaluate the evolution of the triangles (transitivity relation in the network). For\n",
    "this purpose, we consider time increment as an accumulation of one thousand successive\n",
    "rows in the original dataset. Suggest a script that calculates the evolution of the proportion\n",
    "of triangles (number of triangles over the total number of nodes up to time t, basically t=1,\n",
    "2,…,40), and draws the corresponding graph.\n",
    "\n",
    "TODO: Does not work correctly\n",
    "'''\n",
    "\n",
    "def calculate_triangle_evolution(G):\n",
    "    triangles = nx.triangles(G)\n",
    "    num_triangles = sum(triangles.values())\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    proportions = []\n",
    "    for i in range(1, 41):\n",
    "        subgraph = G.subgraph(list(G.nodes())[:i*1000])\n",
    "        triangles = nx.triangles(subgraph)\n",
    "        num_triangles = sum(triangles.values())\n",
    "        proportion = num_triangles / num_nodes\n",
    "        proportions.append(proportion)\n",
    "\n",
    "    plt.plot(range(1, 41), proportions)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Proportion of triangles\")\n",
    "    plt.title(\"Evolution of the proportion of triangles\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "calculate_triangle_evolution(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "10. From 9), write a script that identifies instances of triplet nodes A, B, C such at time t A is\n",
    "connected to B and B is connected to C but A is not connected to C, while in time t+1, A\n",
    "becomes connected to C. Check for theses instances whether the link prediction using\n",
    "common neighbor (probability A being connected to C increases with the number of\n",
    "common neighbours between A and B). Write the result in a table.\n",
    "\n",
    "TODO: Does not work correctly\n",
    "'''\n",
    "\n",
    "def link_prediction(G):\n",
    "    # Identify instances of triplet nodes A, B, C\n",
    "    instances = []\n",
    "    for i in range(1, 41):\n",
    "        subgraph = G.subgraph(list(G.nodes())[:i*1000])\n",
    "        for node in subgraph.nodes():\n",
    "            neighbors = list(subgraph.neighbors(node))\n",
    "            for neighbor in neighbors:\n",
    "                other_nodes = [n for n in subgraph.nodes() if n != node and n != neighbor]\n",
    "                for other_node in other_nodes:\n",
    "                    if other_node not in neighbors and other_node in list(subgraph.neighbors(neighbor)):\n",
    "                        instances.append((node, neighbor, other_node))\n",
    "\n",
    "    # Check if A becomes connected to C at time t+1\n",
    "    rows = []\n",
    "    for instance in instances:\n",
    "        A, B, C = instance\n",
    "        if G.has_edge(A, C):\n",
    "            continue\n",
    "        if G.has_edge(A, B) and G.has_edge(B, C):\n",
    "            common_neighbors = list(nx.common_neighbors(G, A, C))\n",
    "            row = pd.DataFrame({'A': [A], 'B': [B], 'C': [C], 'Common Neighbors': [len(common_neighbors)]})\n",
    "            rows.append(row)\n",
    "    if rows:\n",
    "        table = pd.concat(rows, ignore_index=True)\n",
    "        print(table)\n",
    "    else:\n",
    "        print(\"No instances found\")\n",
    "\n",
    "link_prediction(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "11. Suggest appropriate literature to comment on the various findings and explore the limitation\n",
    "of the reasoning pipeline.\n",
    "\n",
    "TODO:\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
